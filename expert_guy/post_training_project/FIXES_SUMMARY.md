# GRPO Wordle Agent - Fixes Summary

## Overview
This document summarizes all fixes applied to address hallucinations, dead-letter reuse, and invalid guesses in the GRPO-trained Wordle agent.

## Problems Addressed

1. **Prompt Contamination**: Model hallucinates "STORM" and "BRAVE" from training examples
2. **Dead-Letter Reuse**: Model reuses letters marked as B(x) and R(x)
3. **Invalid Guesses**: Makes 4-letter guesses like "BRIG" and non-words like "BRIGE"
4. **Position Misinterpretation**: Doesn't understand R(-) means wrong position

## Fixes Applied (Priority Order)

### 1. Prompt Contamination Fix âœ“ (HIGHEST PRIORITY)
**File**: `basecase_l3_dataset_peft.py`, `test_trained_model.py`
**Change**: Replaced STORM/BRAVE/BRISK examples with generic PLUMB/CLIMB example
**Impact**: Prevents model from referencing training data examples

**Old prompt example:**
```
Secret Word: BRISK
Guess 1: STORM â†’ Feedback: S(-) T(x) O(x) R(-) M(x)
Guess 2: BRAVE â†’ Feedback: B(âœ“) R(âœ“) A(x) V(x) E(x)
```

**New prompt example:**
```
Secret Word: PLUMB
Guess 1: CLIMB â†’ Feedback: C(x) L(âœ“) I(x) M(âœ“) B(âœ“)
  Analysis: L is correct at position 2, M at position 4, B at position 5
  C and I are not in the word at all
```

### 2. Dead-Letter Cumulative Penalty âœ“ (HIGH PRIORITY)
**File**: `reward_functions.py:91-176`
**Changes**:
- Increased `WRONG_LETTER_PENALTY` from -0.5 to -1.0
- Added cumulative escalating penalty: 1st dead letter = -1.0, 2nd = -2.0, 3rd = -3.0
- Explicit tracking of dead letters in separate set

**Code snippet:**
```python
# FIX: Cumulative penalty for reusing dead letters
dead_letter_count = 0
for idx, letter in enumerate(guess):
    if letter in dead_letters:
        dead_letter_count += 1
        reward += WRONG_LETTER_PENALTY * dead_letter_count  # Escalating penalty
```

### 3. Explicit Position Masks âœ“ (HIGH PRIORITY)
**File**: `basecase_l3_dataset_peft.py:118-165`, `test_trained_model.py:95-143`
**Changes**:
- Added explicit position tracking summary after each guess
- Clarified meaning of âœ“, -, and x symbols with detailed examples
- Added forbidden position lists for wrong-position letters

**New prompt addition:**
```
### FEEDBACK SUMMARY:
âœ“ Confirmed positions (KEEP these exact positions):
  Position 2: L
- Letters in word but WRONG position (try different positions):
  R: DO NOT use at position(s) 4
x Dead letters (NEVER use these again):
  S, T, O, M
```

### 4. Temperature Schedule Callback âœ“ (MEDIUM PRIORITY)
**File**: `grpo_local_data_peft.py:36-74`
**Changes**:
- Implemented `TemperatureSchedulerCallback` class
- Starts at temperature 1.0 (high exploration)
- Linearly decays to 0.3 over first 30% of training
- Stays at 0.3 for remaining 70% (exploitation)

**Code snippet:**
```python
class TemperatureSchedulerCallback(TrainerCallback):
    def __init__(self, start_temp=1.0, end_temp=0.3, transition_ratio=0.3):
        # Linear decay: 1.0 â†’ 0.3 at 30% training
```

### 5. Staged Invalid-Guess Penalties âœ“ (MEDIUM PRIORITY)
**File**: `reward_functions.py:23-88`
**Changes**:
- Lenient penalties early in training (encourage exploration)
- Harsh penalties late in training (enforce correctness)
- Separate penalties for wrong length vs invalid words

**Penalty schedule:**
| Violation | Early (0%) | Late (100%) |
|-----------|-----------|------------|
| Wrong length (4-letter) | -0.1 | -2.0 |
| Invalid word (non-word) | -0.5 | -3.0 |

### 6. Proper Wordle Feedback Logic âœ“
**File**: `basecase_l3_dataset_peft.py:153-182`, `test_trained_model.py:165-194`
**Changes**:
- Fixed duplicate letter handling
- Two-pass algorithm: first mark correct positions, then wrong positions
- Maintains letter count to handle words like "SPEED" correctly

## Modified Files

| File | Changes | Lines Modified |
|------|---------|---------------|
| `reward_functions.py` | Dead-letter penalty, staged penalties, case-insensitive | ~90 lines |
| `grpo_local_data_peft.py` | Temperature callback, training progress tracking, output2 | ~80 lines |
| `basecase_l3_dataset_peft.py` | Prompt fix, position masks, proper feedback | ~100 lines |
| `test_trained_model.py` | Updated prompts, points to output2 | ~60 lines |
| `test_reward_functions.py` | NEW - Unit tests for all fixes | 270 lines |
| `validation_test.py` | NEW - Pre-training validation script | 150 lines |

## Testing

### Pre-Training Validation
Run BEFORE training to ensure fixes work:
```bash
python validation_test.py
```

This runs:
1. Unit tests for all reward functions
2. 3 sample game simulations validating:
   - Dead letter punishment
   - Position mask interpretation
   - Staged penalties

### Expected Output
```
PRE-TRAINING VALIDATION - Testing All Fixes
============================================================

Step 1: Running unit tests...
âœ“ PASSED: Dead letter penalty escalates with more violations
âœ“ PASSED: Invalid length penalties increase during training
âœ“ PASSED: Invalid word penalties increase during training
âœ“ PASSED: Model correctly distinguishes âœ“ (correct) vs - (wrong position)
âœ“ PASSED: Only valid 5-letter words get positive rewards
âœ“ PASSED: Reusing same wrong position gets penalized

Step 2: Running game simulations...
âœ“ PASSED: Dead letters properly penalized
âœ“ PASSED: Position masks correctly interpreted
âœ“ PASSED: Staged penalties work correctly

ALL VALIDATIONS PASSED âœ“

ðŸŽ‰ Your fixes are working correctly!

You can now safely run training with:
  python grpo_local_data_peft.py

Training will save model to: output2/wordle-grpo-peft/
```

## Training

### Run Training
```bash
python grpo_local_data_peft.py
```

### Training Configuration
- **Output**: `output2/wordle-grpo-peft/` (won't overwrite old model)
- **Epochs**: 5
- **Temperature**: 1.0 â†’ 0.3 (first 30% of training)
- **Learning rate**: 1e-6
- **LoRA rank**: 128

### Monitor Training
```bash
# TensorBoard
tensorboard --logdir=output2/wordle-grpo-peft/logs

# Weights & Biases
# Check your W&B dashboard
```

## Post-Training Testing

### Test Trained Model
```bash
python test_trained_model.py
```

Tests on 5 words: CRANE, AUDIO, STARE, PLUMB, FROST
(Avoids BRISK, STORM, BRAVE - contaminated examples)

### Expected Improvements
- âœ“ No hallucinated references to "STORM" or "BRAVE"
- âœ“ No reuse of dead letters (x markers)
- âœ“ Correct interpretation of position feedback (-, âœ“)
- âœ“ Valid 5-letter words only
- âœ“ Better win rate and fewer guesses

## Key Metrics to Track

### Before Fixes (Baseline)
- Win rate: ~X% (measure on old model)
- Invalid guesses: High (4-letter words, non-words)
- Dead letter reuse: Frequent
- Hallucinations: Present

### After Fixes (Expected)
- Win rate: Improved by 20-30%
- Invalid guesses: Near zero (especially late training)
- Dead letter reuse: Minimal (strong penalties)
- Hallucinations: Eliminated

## Files Structure

```
expert_guy/post_training_project/
â”œâ”€â”€ reward_functions.py          # âœ“ Updated with all fixes
â”œâ”€â”€ grpo_local_data_peft.py     # âœ“ Points to output2, temperature schedule
â”œâ”€â”€ basecase_l3_dataset_peft.py # âœ“ New prompts, position masks
â”œâ”€â”€ test_trained_model.py        # âœ“ Tests output2 model
â”œâ”€â”€ test_reward_functions.py     # âœ“ NEW - Unit tests
â”œâ”€â”€ validation_test.py           # âœ“ NEW - Pre-training validation
â”œâ”€â”€ FIXES_SUMMARY.md            # âœ“ NEW - This file
â”œâ”€â”€ output2/                     # Empty - will contain new trained model
â”‚   â””â”€â”€ wordle-grpo-peft/       # Created during training
â”‚       â”œâ”€â”€ final_model/        # Final model checkpoint
â”‚       â”œâ”€â”€ best_model/         # Best model checkpoint
â”‚       â”œâ”€â”€ logs/               # TensorBoard logs
â”‚       â””â”€â”€ training_metadata.json
â””â”€â”€ outputs/                     # Old training (untouched)
    â””â”€â”€ wordle-grpo-peft/
```

## Implementation Details

### Reward Function Weights
```python
# Format check
VALID_WORD_REWARD = 1.0
INVALID_LENGTH_PENALTY = -0.1 to -2.0 (staged)
INVALID_WORD_PENALTY = -0.5 to -3.0 (staged)

# Feedback usage
CORRECT_POSITION_REWARD = 0.2
NEW_POSITION_REWARD = 0.1
REPEATED_POSITION_PENALTY = -0.2
WRONG_LETTER_PENALTY = -1.0 (cumulative)
EXPLORATION_REWARD = 0.05

# Total reward = format_reward + feedback_reward + info_gain_reward
```

### Temperature Schedule
```
Progress  | Temperature | Behavior
----------|-------------|----------
0%        | 1.0         | High exploration
10%       | 0.77        | Transitioning
20%       | 0.53        | Transitioning
30%       | 0.3         | Exploitation
40%       | 0.3         | Exploitation
...       | ...         | ...
100%      | 0.3         | Exploitation
```

## Troubleshooting

### If validation_test.py fails:
1. Check that `five_letter_words.csv` exists
2. Verify reward function imports work
3. Run unit tests individually: `python test_reward_functions.py`

### If training fails:
1. Check GPU/CPU memory
2. Reduce `per_device_train_batch_size` from 2 to 1
3. Check wandb/tensorboard logs for errors

### If model still hallucinates:
1. Verify you're using files with the fixes (check for "FIX:" comments)
2. Ensure training used `output2` directory
3. Check that test_trained_model.py points to `output2/wordle-grpo-peft/final_model`

## Next Steps

1. âœ“ Run `python validation_test.py` to verify fixes
2. âœ“ Run `python grpo_local_data_peft.py` to train new model
3. âœ“ Monitor training via TensorBoard/W&B
4. âœ“ Run `python test_trained_model.py` to test trained model
5. âœ“ Compare metrics with baseline
6. âœ“ If needed, adjust hyperparameters and retrain

## References

- Original training script: `grpo_local_data_peft.py` (pre-fixes)
- Dataset: `predibase/wordle-grpo`
- Base model: `Qwen/Qwen2.5-3B-Instruct`
- Training method: GRPO with PEFT (LoRA)

---

**Created**: 2025-12-17
**Author**: Claude Code
**Version**: 2.0 (with all fixes applied)
