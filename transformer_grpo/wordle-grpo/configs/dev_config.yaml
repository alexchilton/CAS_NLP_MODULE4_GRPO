


# Development Configuration for Wordle GRPO Training (Mac testing)
# Updated with more data, epochs, and few-shot prompts

training:
  batch_size: 1
  num_generations: 2
  max_samples: 100  # Increased from 10 to 100
  gradient_accumulation_steps: 4
  epochs: 3  # Increased from 1 to 3
  learning_rate: 5e-5

model:
  name: "gpt2"  # Options: "gpt2", "meta-llama/Llama-3.2-3B", etc.
  use_quantization: false
  lora_rank: 8
  lora_alpha: 16
  # target_modules: auto-detected based on model architecture (can override here if needed)

data:
  dataset_name: "predibase/wordle-grpo"
  train_split: "train[:100]"  # Use 100 training samples
  word_list_path: "data/wordle_word_list.csv"

output:
  checkpoint_dir: "./checkpoints_dev"
  log_dir: "./logs_dev"
